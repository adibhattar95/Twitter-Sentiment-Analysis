{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraires\n",
    "import spacy\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from afinn import Afinn\n",
    "from string import punctuation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and view dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to string\n",
    "df['tweet'] = df['tweet'].astype('str')\n",
    "\n",
    "#Change encoding of df['tweet']\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.encode('ascii', 'ignore'))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower case\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert tweets into list\n",
    "tweets = df['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View all hashtags in the provided tweets\n",
    "hashtags = []\n",
    "for tweet in tweets:\n",
    "    hashtag = re.findall(r\"#(\\w+)\", tweet)\n",
    "    hashtags.append(hashtag)\n",
    "    \n",
    "flat_list = [item for sublist in hashtags for item in sublist]\n",
    "flat_list = [x.lower() for x in flat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique list of hashtags\n",
    "hashtags_unique = []\n",
    "for x in flat_list:\n",
    "    if x not in hashtags_unique:\n",
    "        hashtags_unique.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize an afinn lexicon\n",
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the sentiment score for each hashtag\n",
    "afinn_scores = [afinn.score(text) for text in hashtags_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a df of afinn scores\n",
    "afinn_df = pd.DataFrame(afinn_scores)\n",
    "afinn_df['hashtags'] = hashtags_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UnImportant hashtags\n",
    "afinn_df = afinn_df[afinn_df[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags to include as stopwords\n",
    "hashtag_stop = afinn_df['hashtags'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove user handles\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for match in r:\n",
    "        input_txt = re.sub(match, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<ipython-input-40-30ebf68acb4f>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "  df['clean_tweets'] = df['tweet'].apply(lambda row: remove_pattern(row, \"@[\\w]*\"))\n"
     ]
    }
   ],
   "source": [
    "#Remove user handles\n",
    "df['clean_tweets'] = df['tweet'].apply(lambda row: remove_pattern(row, \"@[\\w]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make list of stopwords\n",
    "stop_words = list(punctuation) + list([\"{link}\"])\n",
    "stop_words.append('link')\n",
    "stop_words.append('amp')\n",
    "stop_words.append(\"'s\")\n",
    "stop_words.append(\".\")\n",
    "stop_words.append(\"'s\")\n",
    "stop_words.append(\"_\")\n",
    "stop_words.append(\"'s\")\n",
    "stop_words.append(\"/\")\n",
    "stop_words.append(\"rt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words with less than 3 characters\n",
    "#df['clean_tweets'] = df['clean_tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "df['tokenized_text'] = df['clean_tweets'].apply(lambda row: nltk.word_tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword removal\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda row : [word for word in row if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a lemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemma = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize the tweets\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [stemma.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into bag of words\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       sxswnui sxsw appl defin languag of touch with ...\n",
       "1       learn ab googl doodl all doodl should be light...\n",
       "2       one of the most in-your-fac ex of steal the sh...\n",
       "3       thi iphon sxsw app would b pretti awesom if it...\n",
       "4       line outsid the appl store in austin wait for ...\n",
       "5       technew one lone dude await ipad 2 at appl sxs...\n",
       "6       sxsw tip princ npr video toy shop with zuckerb...\n",
       "7       nu user new ubersoci for iphon now in the app ...\n",
       "8                       free sxsw sampler on itun freemus\n",
       "9       i think i might go all weekend without see the...\n",
       "10      offici sxsw app sxsw go bit.ly/hmiiga android ...\n",
       "11                   it offici i 'm buy an ipad sxsw elev\n",
       "12      they 're give away ipad 2 x box and book at sx...\n",
       "13      we 're offici at sxsw come by the grill mentio...\n",
       "14      compani to watch from the sxsw trade show floo...\n",
       "15      googl marissa mayer futur of locat augment rea...\n",
       "16      dl the calyp app to get into calyp casa at sxs...\n",
       "17      well yeah music gt iphon nerd my god cute girl...\n",
       "18      appl open pop up store at sxsw via appl ipad2 ...\n",
       "19      atleast you are at sxsw i 'm not there and i h...\n",
       "20               cue the choir music sxsw appl store sxsw\n",
       "21      anyon at sxsw want an ipad 2 i 'm in line and ...\n",
       "22      ____ ___ ______ ... gt gt googl to launch majo...\n",
       "23      p. and googl throw a b tchin parti shout out t...\n",
       "24      i have a 3g iphon after 3 hr tweet at rise_aus...\n",
       "25      doe anyon know if googl did talk about circl a...\n",
       "26      googl set to launch new social network circl t...\n",
       "27      googl to launch major new social network call ...\n",
       "28      stand on a long line surround by unemploy tech...\n",
       "29      wow *someth ventured* wa a kick ass film ... g...\n",
       "                              ...                        \n",
       "7244    googl look to the futur with mobil location-ba...\n",
       "7245    fantastico marissa mayer googl will connect th...\n",
       "7246    an appl pop-uitp store at sxsw there goe my im...\n",
       "7247    wish he could be at the parti at sxsw instead ...\n",
       "7248    tomorrow charl chen will be speak about androi...\n",
       "7249    z7 lead do n't follow code valid 4:00-7:59:59p...\n",
       "7250    appl set up 5,000-square-foot temporari store ...\n",
       "7251    sxsw gear bag ipad 2 iphon mophi travel light ...\n",
       "7252    viagra for your commun launch android app at s...\n",
       "7253    interest googl to launch major new social netw...\n",
       "7254    googl to launch major new social network call ...\n",
       "7255    sweet appl open a pop-up shop in the scarbroug...\n",
       "7256    the futur is about network not just data that ...\n",
       "7257    did you miss googl vp of search marissa mayer ...\n",
       "7258    massiv line at sxsw appl store ... agre i can ...\n",
       "7259    explor the world in 3d with xml combin googl e...\n",
       "7260                        ipad hipster austincrowd sxsw\n",
       "7261    the new whrrl app is now live in the iphon app...\n",
       "7262    come see someth new about googl sketchup pro a...\n",
       "7263    there are two appl store in atx the ipad 2 goe...\n",
       "7264    head over to ballroom b 'your mom ha an ipad-d...\n",
       "7265    at sxsw appl school the market expert cnet blo...\n",
       "7266    great stuff on fri sxsw marissa mayer googl ti...\n",
       "7267    the session designingforkid is chang my mind a...\n",
       "7268    great visualis of the ghost movement logic in ...\n",
       "7269    googl plze tammi i 'm in middl of sxsw crazi a...\n",
       "7270    are you all set edchat musedchat sxsw sxswi ne...\n",
       "7271    aha found proof of lactat room excus me quot m...\n",
       "7272    we just launch our ipad app at sxsw get all th...\n",
       "7273    the next fin serv battl is vs appl goog mobil ...\n",
       "Name: tokenized_text, Length: 7274, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "df = df.drop(['tweet', 'clean_tweets'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file\n",
    "df.to_csv('train_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
