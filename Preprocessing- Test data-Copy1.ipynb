{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraires\n",
    "import spacy\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from afinn import Afinn\n",
    "from string import punctuation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                              tweet  sentiment\n",
       "0      1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1      1851  Learning ab Google doodles! All doodles should...          1\n",
       "2      2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3      4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4      3604  Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and view dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to string\n",
    "df['tweet'] = df['tweet'].astype('str')\n",
    "\n",
    "#Change encoding of df['tweet']\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.encode('ascii', 'ignore'))\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower case\n",
    "df['tweet'] = df['tweet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert tweets into list\n",
    "tweets = df['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View all hashtags in the provided tweets\n",
    "hashtags = []\n",
    "for tweet in tweets:\n",
    "    hashtag = re.findall(r\"#(\\w+)\", tweet)\n",
    "    hashtags.append(hashtag)\n",
    "    \n",
    "flat_list = [item for sublist in hashtags for item in sublist]\n",
    "flat_list = [x.lower() for x in flat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique list of hashtags\n",
    "hashtags_unique = []\n",
    "for x in flat_list:\n",
    "    if x not in hashtags_unique:\n",
    "        hashtags_unique.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize an afinn lexicon\n",
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the sentiment score for each hashtag\n",
    "afinn_scores = [afinn.score(text) for text in hashtags_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a df of afinn scores\n",
    "afinn_df = pd.DataFrame(afinn_scores)\n",
    "afinn_df['hashtags'] = hashtags_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UnImportant hashtags\n",
    "afinn_df = afinn_df[afinn_df[0] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags to include as stopwords\n",
    "hashtag_stop = afinn_df['hashtags'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove user handles\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for match in r:\n",
    "        input_txt = re.sub(match, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "<ipython-input-15-30ebf68acb4f>:2: DeprecationWarning: invalid escape sequence \\w\n",
      "  df['clean_tweets'] = df['tweet'].apply(lambda row: remove_pattern(row, \"@[\\w]*\"))\n"
     ]
    }
   ],
   "source": [
    "#Remove user handles\n",
    "df['clean_tweets'] = df['tweet'].apply(lambda row: remove_pattern(row, \"@[\\w]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make list of stopwords\n",
    "stop_words = list(punctuation) + hashtag_stop + list([\"{link}\"])\n",
    "stop_words.append('link')\n",
    "stop_words.append('amp')\n",
    "stop_words.append(\"'s\")\n",
    "stop_words.append(\".\")\n",
    "stop_words.append(\"'s\")\n",
    "stop_words.append(\"_\")\n",
    "stop_words.append(\"'s\")\n",
    "stop_words.append(\"/\")\n",
    "stop_words.append(\"rt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words with less than 3 characters\n",
    "#df['clean_tweets'] = df['clean_tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "df['tokenized_text'] = df['clean_tweets'].apply(lambda row: nltk.word_tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword removal\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda row : [word for word in row if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a lemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize the tweets\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [lemma.lemmatize(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into bag of words\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       defining language of touch with different dial...\n",
       "1       learning ab doodle all doodle should be light ...\n",
       "2       one of the most in-your-face ex of stealing th...\n",
       "3       this would b pretty awesome if it did n't cras...\n",
       "4                             outside the waiting for the\n",
       "5                                    one lone dude awaits\n",
       "6                 tip prince toy shopping with zuckerberg\n",
       "7       nu user for now the includes uberguide to spon...\n",
       "8                                         free sampler on\n",
       "9       think might go all without seeing the same cas...\n",
       "10                              official go bit.ly/hmiiga\n",
       "11                               it official 'm buying an\n",
       "12                         they 're giving away x box and\n",
       "13      we 're officially come by the grill mention w/...\n",
       "14                     to watch from the trade show floor\n",
       "15      marissa future of augmented reality contextual...\n",
       "16      dl the to get into casa the free is available ...\n",
       "17      well yeah gt my god cute girl everywhere ... l...\n",
       "18                                           open pop via\n",
       "19               atleast you are 'm not there and have no\n",
       "20                                          cue the choir\n",
       "21      anyone want an 'm and will pick one for someon...\n",
       "22      ____ ___ ______ ... gt gt to major network cal...\n",
       "23      p.s and throw a b tchin shout out to the spazm...\n",
       "24      have a 3g after 3 hr tweeting it wa dead need ...\n",
       "25                      doe anyone know if did talk about\n",
       "26                                   set to network today\n",
       "27                 to major network called possibly today\n",
       "28      standing on a long surrounded by unemployed te...\n",
       "29      wow *something ventured* wa a kick as ... or b...\n",
       "                              ...                        \n",
       "7244        look to the future with location-based device\n",
       "7245    fantastico marissa will connect the physical w...\n",
       "7246    an pop-uitp there go my control j.mp/i41h53 lt...\n",
       "7247    wish he could be the instead of doing homework...\n",
       "7248    tomorrow charles chen will be speaking about a...\n",
       "7249    z7 lead do n't follow code valid 4:00-7:59:59p...\n",
       "7250    set 5,000-square-foot temporary to sell test p...\n",
       "7251                   gear bag traveling light this year\n",
       "7252    viagra for your communication launch happy to ...\n",
       "7253    interesting to major network called possibly t...\n",
       "7254               to major network called possibly today\n",
       "7255    sweet opening a pop-up shop the scarbrough bui...\n",
       "7256    the future is about not just data that why may...\n",
       "7257    did you miss vp of marissa last week listen to...\n",
       "7258    massive ... agree can wait perhaps late early ...\n",
       "7259    exploring the world 3d with xml combine earth ...\n",
       "7260                                                     \n",
       "7261    the whrrl is now live the and marketplace get ...\n",
       "7262    come see something about sketchup pro via sket...\n",
       "7263    there are two store the go on next friday ... ...\n",
       "7264    heading over to ballroom b 'your mom ha an ipa...\n",
       "7265                               school the expert blog\n",
       "7266    great stuff on fri marissa tim o'reilly books/...\n",
       "7267    the session is changing my mind about my futur...\n",
       "7268    great visualisation of the ghost movement logi...\n",
       "7269    plze tammi 'm middle of craziness and everythi...\n",
       "7270                                      are you all set\n",
       "7271    aha found proof of lactation room excuse me qu...\n",
       "7272    we just launched our get all the detail the fi...\n",
       "7273    the next fin serv battle is v goog operator th...\n",
       "Name: tokenized_text, Length: 7274, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "df = df.drop(['tweet', 'clean_tweets'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file\n",
    "df.to_csv('clean_train4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
